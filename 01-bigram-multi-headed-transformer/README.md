# Bigram Multi-Headed Transformer Model

Model 01 is a Multi-Headed Transformer model using character tokens

## Features

- **Transformer Architecture**: We use masked attention as per usual
- **Multi-Headed Attention**: The K and V matrices are split up to a moderate number of attention heads
- **Exact Vocabulary**: Prior to first training run the source data is scanned for all unique tokens

## Performance


